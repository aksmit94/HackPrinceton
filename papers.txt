  this paper presents sampling-based speech parameter generation using moment-matching networks for deep neural network (dnn)-based speech synthesis. although people never produce exactly the same speech even if we try to express the same linguistic and para-linguistic information, typical statistical speech synthesis produces completely the same speech, i.e., there is no inter-utterance variation in synthetic speech. to give synthetic speech natural inter-utterance variation, this paper builds dnn acoustic models that make it possible to randomly sample speech parameters. the dnns are trained so that they make the moments of generated speech parameters close to those of natural speech parameters. since the variation of speech parameters is compressed into a low-dimensional simple prior noise vector, our algorithm has lower computation cost than direct sampling of speech parameters. as the first step towards generating synthetic speech that has natural inter-utterance variation, this paper investigates whether or not the proposed sampling-based generation deteriorates synthetic speech quality. in evaluation, we compare speech quality of conventional maximum likelihood-based generation and proposed sampling-based generation. the result demonstrates the proposed generation causes no degradation in speech quality. 
  we present a single-channel phase-sensitive speech enhancement algorithm that is based on modulation-domain kalman filtering and on tracking the speech phase using circular statistics. with kalman filtering, using that speech and noise are additive in the complex stft domain, the algorithm tracks the speech log-spectrum, the noise log-spectrum and the speech phase. joint amplitude and phase estimation of speech is performed. given the noisy speech signal, conventional algorithms use the noisy phase for signal reconstruction approximating the speech phase with the noisy phase. in the proposed kalman filtering algorithm, the speech phase posterior is used to create an enhanced speech phase spectrum for signal reconstruction. the kalman filter prediction models the temporalinter-frame correlation of the speech and noise log-spectra and of the speech phase, while the kalman filter update models their nonlinear relations. with the proposed algorithm, speech is tracked and estimated both in the log-spectral and spectral phase domains. the algorithm is evaluated in terms of speech quality and different algorithm configurations, dependent on the signal model, are compared in different noise types. experimental results show that the proposed algorithm outperforms traditional enhancement algorithms over a range of snrs for various noise types. 
  in this paper we consider the problem of speech enhancement in real-world like conditions where multiple noises can simultaneously corrupt speech. most of the current literature on speech enhancement focus primarily on presence of single noise in corrupted speech which is far from real-world environments. specifically, we deal with improving speech quality in office environment where multiple stationary as well as non-stationary noises can be simultaneously present in speech. we propose several strategies based on deep neural networks (dnn) for speech enhancement in these scenarios. we also investigate a dnn training strategy based on psychoacoustic models from speech coding for enhancement of noisy speech 
  form about four decades human beings have been dreaming of an intelligent machine which can master the natural speech. in its simplest form, this machine should consist of two subsystems, namely automatic  (asr) and speech understanding (su). the goal of asr is to transcribe natural speech while su is to understand the meaning of the transcription. recognizing and understanding a spoken sentence is obviously a knowledge-intensive process, which must take into account all variable information about the speech communication process, from acoustics to semantics and pragmatics. while developing an automatic  system, it is observed that some adverse conditions degrade the performance of the  system. in this contribution, speech enhancement system is introduced for enhancing speech signals corrupted by additive noise and improving the performance of automatic speech recognizers in noisy conditions. automatic  experiments show that replacing noisy speech signals by the corresponding enhanced speech signals leads to an improvement in the recognition accuracies. the amount of improvement varies with the type of the corrupting noise. 
  speech synthesis is widely used in many practical applications. in recent years, speech synthesis technology has developed rapidly. however, one of the reasons why synthetic speech is unnatural is that it often has over-smoothness. in order to improve the naturalness of synthetic speech, we first extract the mel-spectrogram of speech and convert it into a real image, then take the over-smooth mel-spectrogram image as input, and use image-to-image translation generative adversarial networks(gans) framework to generate a more realistic mel-spectrogram. finally, the results show that this method greatly reduces the over-smoothness of synthesized speech and is more close to the mel-spectrogram of real speech. 
  mel frequency cepstral coefficients (mfccs) are the most popularly used speech features in most speech and speaker recognition applications. in this paper, we study the effect of resampling a speech signal on these speech features. we first derive a relationship between the mfcc param- eters of the resampled speech and the mfcc parameters of the original speech. we propose six methods of calculating the mfcc parameters of downsampled speech by transforming the mel filter bank used to com- pute mfcc of the original speech. we then experimentally compute the mfcc parameters of the down sampled speech using the proposed meth- ods and compute the pearson coefficient between the mfcc parameters of the downsampled speech and that of the original speech to identify the most effective choice of mel-filter band that enables the computed mfcc of the resampled speech to be as close as possible to the original speech sample mfcc. 
  precise detection of speech endpoints is an important factor which affects the performance of the systems where speech utterances need to be extracted from the speech signal such as automatic  (asr) system. existing endpoint detection (epd) methods mostly uses short-term energy (ste), zero-crossing rate (zcr) based approaches and their variants. but ste and zcr based epd algorithms often fail in the presence of non-speech sound artifacts (nsas) produced by the speakers. algorithms based on pattern recognition and classification techniques are also proposed but require labeled data for training. a new algorithm termed as wavelet convolution based speech endpoint detection (wcsepd) is proposed in this article to extract speech endpoints. wcsepd decomposes the speech signal into high-frequency and low-frequency components using wavelet convolution and computes entropy based thresholds for the two frequency components. the low-frequency thresholds are used to extract voiced speech segments, whereas the high-frequency thresholds are used to extract the unvoiced speech segments by filtering out the nsas. wcsepd does not require any labeled data for training and can automatically extract speech segments. experiment results show that the proposed algorithm precisely extracts speech endpoints in the presence of nsas. 
  this paper presents a new approach for classification of dysfluent and fluent speech using mel-frequency cepstral coefficient (mfcc). the speech is fluent when person's speech flows easily and smoothly. sounds combine into syllable, syllables mix together into words and words link into sentences with little effort. when someone's speech is dysfluent, it is irregular and does not flow effortlessly. therefore, a dysfluency is a break in the smooth, meaningful flow of speech. stuttering is one such disorder in which the fluent flow of speech is disrupted by occurrences of dysfluencies such as repetitions, prolongations, interjections and so on. in this work we have considered three types of dysfluencies such as repetition, prolongation and interjection to characterize dysfluent speech. after obtaining dysfluent and fluent speech, the speech signals are analyzed in order to extract mfcc features. the k-nearest neighbor (k-nn) and support vector machine (svm) classifiers are used to classify the speech as dysfluent and fluent speech. the 80% of the data is used for training and 20% for testing. the average accuracy of 86.67% and 93.34% is obtained for dysfluent and fluent speech respectively. 
  speech recognizers trained on close-talking speech do not generalize to distant speech and the word error rate degradation can be as large as 40% absolute. most studies focus on tackling distant  as a separate problem, leaving little effort to adapting close-talking speech recognizers to distant speech. in this work, we review several approaches from a domain adaptation perspective. these approaches, including speech enhancement, multi-condition training, data augmentation, and autoencoders, all involve a transformation of the data between domains. we conduct experiments on the ami data set, where these approaches can be realized under the same controlled setting. these approaches lead to different amounts of improvement under their respective assumptions. the purpose of this paper is to quantify and characterize the performance gap between the two domains, setting up the basis for studying adaptation of speech recognizers from close-talking speech to distant speech. our results also have implications for improving distant . 
  the paper presents the capability of an hmm-based tts system to produce bengali speech. in this synthesis method, trajectories of speech parameters are generated from the trained hidden markov models. a final speech waveform is synthesized from those speech parameters. in our experiments, spectral properties were represented by mel cepstrum coefficients. both the training and synthesis issues are investigated in this paper using annotated bengali speech database. experimental evaluation depicts that the developed text-to-speech system is capable of producing adequately natural speech in terms of intelligibility and intonation for bengali. 
