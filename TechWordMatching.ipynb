{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "query = 'speech recognition'\n",
    "num_results = 1000\n",
    "url_link = ('http://export.arxiv.org/api/query?search_query=all:' + query + '&start=0&max_results=' + str(num_results))\n",
    "with urllib.request.urlopen(url_link) as url:\n",
    "    data = url.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def gen_syn(word):\n",
    "    synonyms = []\n",
    "    x = (wn.synsets(word))\n",
    "    for syn in x:\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "\n",
    "    x = set(synonyms)\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.decode(\"utf-8\") \n",
    "data = data.lower()\n",
    "data = data.replace('\\n',' ')\n",
    "data = data.replace('$','')\n",
    "data = data.split('summary>')\n",
    "syn_list = gen_syn(query)\n",
    "for syn in syn_list:\n",
    "    data = data.replace(syn,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ''\n",
    "summary = []\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i].replace('<', '')\n",
    "    data[i] = data[i].replace('>', '')\n",
    "    data[i] = data[i].replace('/', '')\n",
    "    data[i] = data[i].replace(query, '')\n",
    "    if (i % 2 == 1):\n",
    "        s += data[i] + '\\n'\n",
    "        summary.append(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "file = open('papers.txt','w')\n",
    "_ = file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('clean_list2.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words)):\n",
    "    words[i] = words[i].replace('\\n','')\n",
    "    words[i] = ' ' + words[i] + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(string):       \n",
    "    counts = {word: 0 for word in words}\n",
    "    \n",
    "    for word in counts:\n",
    "        counts[word] = string.count(word)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = count_words(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in list(counts):\n",
    "    if(word == '' or counts[word] == 0):\n",
    "        counts.pop(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' speech synthesis ': 3,\n",
       " ' pattern recognition ': 1,\n",
       " ' classifiers ': 1,\n",
       " ' k-nearest neighbor ': 1,\n",
       " ' support vector machine ': 1,\n",
       " ' classification ': 2}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
